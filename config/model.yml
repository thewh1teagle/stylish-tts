# Configuration for training on a high-resource (24GB+ VRAM) GPU.

multispeaker: false
n_mels: 80
sample_rate: 24000
n_fft: 2048
win_length: 1200
hop_length: 300
style_dim: 128
inter_dim: 512

text_aligner:
  hidden_dim: 256
  token_embedding_dim: 512

decoder:
  hidden_dim: 1024
  residual_dim: 64

generator:
  type: 'ringformer'
  resblock_kernel_sizes: [ 3, 7, 11 ]
  upsample_rates: [ 4, 5 ]
  upsample_initial_channel: 512
  resblock_dilation_sizes: [ [ 1, 3, 5 ], [ 1, 3, 5 ], [ 1, 3, 5 ] ]
  upsample_kernel_sizes: [ 8, 10 ]
  gen_istft_n_fft: 60
  gen_istft_hop_size: 15
  depth: 2

text_encoder:
  tokens: 178 # number of phoneme tokens
  hidden_dim: 192
  filter_channels: 768
  heads: 2
  layers: 6
  kernel_size: 3
  dropout: 0.1

style_encoder:
  layers: 4

duration_predictor:
  n_layer: 3
  max_dur: 50 # maximum duration of a single phoneme
  dropout: 0.2

pitch_energy_predictor:
  dropout: 0.2

# speech language model config
slm:
  model: 'microsoft/wavlm-base-plus'
  sr: 16000 # sampling rate of SLM

symbol:
  pad: "$"
  punctuation: ";:,.!?¡¿—…\"()“” "
  letters: "ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz"
  letters_ipa: "ɑɐɒæɓʙβɔɕçɗɖðʤəɘɚɛɜɝɞɟʄɡɠɢʛɦɧħɥʜɨɪʝɭɬɫɮʟɱɯɰŋɳɲɴøɵɸθœɶʘɹɺɾɻʀʁɽʂʃʈʧʉʊʋⱱʌɣɤʍχʎʏʑʐʒʔʡʕʢǀǁᵊǃˈˌːˑʼʴʰʱʲʷˠˤ˞↓↑→↗↘'̩'ᵻ"
